## 爬虫整体流程

1. 重写入口函数 start_requests (不管start_urls里面写的是什么，启动之后的第一件事情就是 寻找start_requests)
2. 在start_requests 里面我们可以使用FormRequest模拟登陆,
   或者 直接添加cookie进行登陆
3. 

4. 我们要从写mysqlpipeline 
   因为我们有两个 item 一个知乎问题， 一个知乎答案。
   不对 我们还有一个伯乐在线的item， 一共三个
   三个都要经过mysqlpipeline
   如果还有其他的 怎么办？ 一个item写一个pipeline？ 有100个呢？写100个？

   **我们只需要根据不同的item修改do_insert()里面的sql语句**
   if item.__class__.__name__ == 'JobBoleArticleItem':
       insert_sql = '''xxxxxx'''
   **如果我们后期把我们的Item的名字改了那么这个就不能用了
   	 我们该怎么做呢？
   
   我们借助Django的model， 借助model的class 可以直接save
   这样我们就屏蔽了具体的sql语句。
   我们借鉴这种思路。 把插入语句都写到我们的item里面
```python

# 在item中添加get_sql()函数
class BoleArticle(Item):
    title = Field(
        input_processor = MapCompose(add_shuiyin)
    )
    url = Field()
    url_md5_id = Field()
    # 这个返回的是一个列表
    # ** 需要注意的是 在写入数据库的时候 我们要提取第一个
    # 在Imagepipeline中 改写 复用
    front_img_url = Field(
        output_processor = MapCompose(return_value)
    )
    front_img_local_path = Field()
    pub_date = Field(
        input_processor = MapCompose(date_convert)
    )
    cate = Field()
    tags = Field(
        input_processor = MapCompose(clean_comment),
        output_processor = Join(separator=',')
    )
    votetotal = Field(
        input_processor = MapCompose(get_num)
    )
    booktotal = Field(
        input_processor=MapCompose(get_num)
    )
    commenttotal = Field(
        input_processor=MapCompose(get_num)
    )
    content = Field()

    def get_insert_sql(self):
        insert_sql = '''
        insert into jobbole_article (title, pub_date, url, url_md5_id, votetotal, front_img_url) values (%s, %s, %s, %s, %s, %s);
                '''
        params = (self['title'], self['pub_date'], self['url'], self['url_md5_id'], self['votetotal'], self['front_img_url'][0])
        return insert_sql, params


# 在pipeline中直接调用item.get_insert_sql()

# 处理异常的函数必须要加。这个print 以后可以写成log 这么以后有错误可以查看
    def handle_error(self, failure, item, spider):
        # 处理异步插入的异常
        print(failure)

    def do_insert(self, cursor, item):
        # 执行具体的插入
#         insert_sql = '''
# insert into jobbole_article (title,pub_date,url,url_md5_id, votetotal, front_img_url) values (%s, %s, %s, %s, %s, %s);
#         '''
#         cursor.execute(insert_sql,
#                        (item['title'], item['pub_date'], item['url'], item['url_md5_id'], item['votetotal'], item['front_img_url'][0])
#                       )
        insert_sql, params = item.get_insert_sql()
        cursor.execute(insert_sql, params)
```
**这样我们在每个item定义的时候加上 get_insert_sql 函数就行了**

## 调试answer的时候 把 question yield 之后直接break掉
## 把yield question_item 也注释掉
```python
    def parse(self, response):
        # 提取页面所有的href
        all_urls = response.css('a::attr(href)').getall()
        all_urls = [response.urljoin(i) for i in all_urls]
        # 下面两种方法都可以过滤
        all_urls = filter(lambda x:True if x.startswith('https') else False, all_urls)
        # all_urls = [i for i in all_urls if i.startswith('https')]
        for url in all_urls:
            match_obj = re.match('(.*zhihu.com/question/(\d+))(/|$).*', url)
            # 如果搜索到 问题 相关链接 则提取
            if match_obj:
                request_url = match_obj.group(1)
                question_id = match_obj.group(2)
                print(request_url, question_id)

                yield scrapy.Request(url=request_url, headers=self.headers, callback=self.parse_question)
                # 调试的时候直接break掉， 异步有可能返回的数据不一样
                break
            # 如果不是 就进入这个页面 继续跟踪
            else:
                pass
                # yield scrapy.Request(url=url, headers=self.headers, callback=self.parse)


```


## 插入数据库有可能主键冲突！！！
解决办法： 查询主键有没有 没有就插入， 有就我们就更新里面的某些字段
mysql 的用法 ON DUPLICATE KEY UPDATE
```python 
insert_sql = '''
        insert into zhihu_question (zhihu_id, topics, url, title, content, answer_num, comments_num, 
        watch_user_num, click_num, crawl_time) 
        values (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        ON DUPLICATE KEY UPDATE content=VALUES(content), answer_num=VALUES(answer_num),
        comment_nums=VALUES(comments_num), watch_user_num=VALUES(watch_user_num), click_num=VALUES(click_num)
        ;
                '''
```


# 我们爬取过快的话 知乎会很快给我们返回403错误的
**错误是知乎的反爬机制， 会检查我们的User-Agent。 同一个User-Agent在同一个ip下请求过于频繁的话。会给我返回403错误界面

**后面反爬机制再讲解， 登录次数过多，知乎返回验证码我们也会介绍