1. selenium模拟登陆
2. selenium 执行js 滚动到最下面
```python
lenOfPage = driver.execute_script("window.scrollTo(0,document.body.scrollHeight); var lenOfPage=document.body.scrollHeight; return lenOfPage")
```
另一种方法 爵爷的方法 暴力 向下滚动多少次
```python
from selenium.webdriver.common.keys import Keys
for i in range(15):
	driver.sendkeys(Keys.END)
```
3. 设置chrome driver 不加载图片 固定写法
```python
options = webdriver.ChromeOptions()
prefs = {"profile.managed_default_content_settings.images":2}
options.add_experimental_option("prefs",prefs)
driver = webdriver.Chrome(options=options)
```
4. 如何把selenium集成到我们的scrapy中 在下载中间件中添加
**这样写 这样写我们爬虫结束的时候 浏览器并不会关闭
```python
from selenium import webdriver
import time
from scrapy.http import HtmlResponse

class JSPageMiddleware(object):
    # 这样写我们爬虫结束的时候 浏览器并不会关闭
    # 我们 直接将 driver的初始化 写到我们的spider下
    def __init__(self):
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        self.driver = webdriver.Chrome(options=options)
        super(JSPageMiddleware, self).__init__()

    # 下面的会有一个问题， 就是每次调用都会启动一个浏览器 我们要添加init
    def process_request(self, request, spider):
        # 我们这里并不是所有的爬虫都用到 js加载的
        # 可以指定spider.name 或者 request.url
        if spider.name == 'jobbole':
        # if request.url xxxx

            self.driver.get(request.url)
            time.sleep(3)
            print(f"当前访问的url是:{request.url}")
            # 这里直接return HtmlResponse
            # 一旦有了HtmlResponse scrapy 就不会再经过中间件，会直接返回给我们的spider
            # 默认的encoding 是 ASCII 码 我们需要根据网页自己写进去， 有可能是utf-8 gbk-2312 等等
            # request 字段必须加进去
            return HtmlResponse(url=self.driver.current_url, body=self.driver.page_source, encoding='utf-8',request=request)
            # 记得写完添加到settings里面的 DownloadMiddler
```

**改写 初始化写在spider里面
```python
# spider
class JobboleSpider(scrapy.Spider):
    name = 'jobbole'
    allowed_domains = ['jobbole.com']
    start_urls = ['http://blog.jobbole.com/all-posts/']

    def __init__(self):
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        self.driver = webdriver.Chrome(options=options)
        super(JobboleSpider, self).__init__()

# middleware
import time
from scrapy.http import HtmlResponse

class JSPageMiddleware(object):

    def process_request(self, request, spider):
        # 我们这里并不是所有的爬虫都用到 js加载的
        # 可以指定spider.name 或者 request.url
        if spider.name == 'jobbole':
        # if request.url xxxx

            spider.driver.get(request.url)
            time.sleep(3)
            print(f"当前访问的url是:{request.url}")
            # 这里直接return HtmlResponse
            # 一旦有了HtmlResponse scrapy 就不会再经过中间件，会直接返回给我们的spider
            # 默认的encoding 是 ASCII 码 我们需要根据网页自己写进去， 有可能是utf-8 gbk-2312 等等
            # request 字段必须加进去
            return HtmlResponse(url=spider.driver.current_url, body=spider.driver.page_source, encoding='utf-8',request=request)
            # 记得写完添加到settings里面的 DownloadMiddler
```

** scrapy 可以在我们 __init__或者其他时候添加一个信号量
我们这里只用到 一个信号  就是当spider 关闭了之后做什么事
```python 
from scrapy.xlib.pydispatch import dispatcher
from scrapy import signals

from scrapy.xlib.pydispatch import dispatcher
from scrapy import signals


class JobboleSpider(scrapy.Spider):
    name = 'jobbole'
    allowed_domains = ['jobbole.com']
    start_urls = ['http://blog.jobbole.com/all-posts/']

    def __init__(self):
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        self.driver = webdriver.Chrome(options=options)
        super(JobboleSpider, self).__init__()
        # 当spider 关闭的时候我们做什么事情 做的事是一个函数
        dispatcher.connect(self.spider_closed, signals.spider_closed)

    def spider_closed(self, spider):
        # 爬虫退出时 关闭driver
        print("spider closed")
        self.driver.quit()

```


出现一些其他问题，就是 我们的middler是同步的。
就不解决了暂时。。。